# -*- coding: utf-8 -*-
"""creditfraud.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10EF0valY_qlUbM5uTJSur4tn9lPF5260
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

from google.colab import files

uploaded = files.upload()

import pandas as pd

df = pd.read_csv('creditcard.csv')

df.head()

df.info()

df.isnull().sum()

df['Class'].value_counts()

legit = df[df.Class == 0]
fraud = df[df.Class == 1]

legit.Amount.describe()

fraud.Amount.describe()

df.groupby('Class').mean()

legit_sample = legit.sample(n=492)

new_dataset = pd.concat([legit_sample, fraud], axis=0)

new_dataset.head()

new_dataset.tail()

new_dataset['Class'].value_counts()

new_dataset.groupby('Class').mean()

X= new_dataset.drop(columns='Class', axis=1)
y= new_dataset['Class']

print(X)

print(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

#creating a model

import torch
from torch import nn

class NeuralNetwork(nn.Module):
  def __init__(self, input_features, output_featues, hidden_units):
    super().__init__()
    self.linear_layer_stack = nn.Sequential(
        nn.Linear(in_features=input_features, out_features=hidden_units),
        nn.ReLU(),
        nn.Linear(in_features=hidden_units, out_features=hidden_units),
        nn.ReLU(),
        nn.Linear(in_features=hidden_units, out_features=hidden_units), # Added another hidden layer
        nn.ReLU(),
        nn.Linear(in_features=hidden_units, out_features=output_featues)
    )

  def forward(self, x):
    return self.linear_layer_stack(x)

model = NeuralNetwork(30, 1, 256)

model

#creating loss, optimizer, accuracy function

loss_fn = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(params=model.parameters(), lr=0.005) # Increased learning rate

def accuracy_fn(y_true, y_pred):
  correct = torch.eq(y_true, y_pred).sum().item()
  acc = (correct / len(y_pred)) * 100
  return acc

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Fit and transform 'Time' and 'Amount'
df['Time'] = scaler.fit_transform(df['Time'].values.reshape(-1, 1))
df['Amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))

# Re-create the balanced dataset with scaled 'Time' and 'Amount'
legit = df[df.Class == 0]
fraud = df[df.Class == 1]

legit_sample = legit.sample(n=492, random_state=2) # Added random_state for reproducibility
new_dataset = pd.concat([legit_sample, fraud], axis=0)

# Shuffle the new dataset
new_dataset = new_dataset.sample(frac=1, random_state=2).reset_index(drop=True) # Added random_state for reproducibility and reset index

X = new_dataset.drop(columns='Class', axis=1)
y = new_dataset['Class']

# Split the data again with the scaled data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=2)

print("X after scaling and splitting:")
display(X.head())
print("\nX_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)

#running training loop

torch.manual_seed(42)

epochs = 10000 # Increased number of epochs

for epoch in range(epochs):
  model.train()

  # Convert DataFrames to Tensors
  X_train_tensor = torch.from_numpy(X_train.values).float()
  y_train_tensor = torch.from_numpy(y_train.values).float()


  y_logits = model(X_train_tensor).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits))

  loss = loss_fn(y_logits, y_train_tensor)
  acc = accuracy_fn(y_true=y_train_tensor, y_pred=y_pred)

  optimizer.zero_grad()

  loss.backward()

  optimizer.step()

  model.eval()

  with torch.inference_mode():
    # Convert DataFrame to Tensor
    X_test_tensor = torch.from_numpy(X_test.values).float()
    y_test_tensor = torch.from_numpy(y_test.values).float()

    test_logits = model(X_test_tensor).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))

    test_loss = loss_fn(test_logits, y_test_tensor)
    test_acc = accuracy_fn(y_true=y_test_tensor, y_pred=test_pred)

  if epoch % 500 == 0:
    print(f"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%")

from sklearn.metrics import precision_score, recall_score, f1_score

# Set model to evaluation mode
model.eval()

# Make predictions on the test set
with torch.inference_mode():
    X_test_tensor = torch.from_numpy(X_test.values).float()
    y_test_tensor = torch.from_numpy(y_test.values).float()

    test_logits = model(X_test_tensor).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits)) # Changed back to rounding


# Convert predictions and ground truth to numpy arrays for scikit-learn metrics
y_true_np = y_test_tensor.numpy()
y_pred_np = test_pred.numpy()

# Calculate precision, recall, and F1-score
precision = precision_score(y_true_np, y_pred_np)
recall = recall_score(y_true_np, y_pred_np)
f1 = f1_score(y_true_np, y_pred_np)

print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

"""# **Going to use entire data set, instead of the small segment**"""

X = df.drop(columns='Class', axis=1)
y = df['Class']

print(X.shape)
print(y.shape)

#splitting the dataset

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

print("Shapes after splitting the full dataset:")
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

#calculating class weights and instantiating the loss function with these weights to address class imbalance.

from sklearn.utils.class_weight import compute_class_weight
import torch

# Calculate class weights for the imbalanced dataset
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)

# Convert class weights to a PyTorch tensor
class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)

# Instantiate the BCEWithLogitsLoss with class weights
# The pos_weight parameter should be the weight for the positive class (Class=1)
loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor[1])

print(f"Class weights: {class_weights}")
print(f"Class weights tensor: {class_weights_tensor}")
print(f"Loss function with pos_weight: {loss_fn}")

#running training loop with class weights

torch.manual_seed(42)

epochs = 100 # Reduced number of epochs for demonstration on full dataset

for epoch in range(epochs):
  model.train()

  # Convert DataFrames to Tensors
  X_train_tensor = torch.from_numpy(X_train.values).float()
  y_train_tensor = torch.from_numpy(y_train.values).float()


  y_logits = model(X_train_tensor).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits))

  # Use the loss function with class weights
  loss = loss_fn(y_logits, y_train_tensor)
  acc = accuracy_fn(y_true=y_train_tensor, y_pred=y_pred)

  optimizer.zero_grad()

  loss.backward()

  optimizer.step()

  model.eval()

  with torch.inference_mode():
    # Convert DataFrame to Tensor
    X_test_tensor = torch.from_numpy(X_test.values).float()
    y_test_tensor = torch.from_numpy(y_test.values).float()

    test_logits = model(X_test_tensor).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))

    # Use the loss function with class weights for testing as well
    test_loss = loss_fn(test_logits, y_test_tensor)
    test_acc = accuracy_fn(y_true=y_test_tensor, y_pred=test_pred)

  if epoch % 10 == 0: # Print more frequently for fewer epochs
    print(f"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%")

#evaluating the model

from sklearn.metrics import confusion_matrix

# Set model to evaluation mode
model.eval()

# Make predictions on the test set
with torch.inference_mode():
    X_test_tensor = torch.from_numpy(X_test.values).float()
    y_test_tensor = torch.from_numpy(y_test.values).float()

    test_logits = model(X_test_tensor).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))

# Convert predictions and ground truth to numpy arrays for scikit-learn metrics
y_true_np = y_test_tensor.numpy()
y_pred_np = test_pred.numpy()

# Calculate precision, recall, and F1-score
precision = precision_score(y_true_np, y_pred_np)
recall = recall_score(y_true_np, y_pred_np)
f1 = f1_score(y_true_np, y_pred_np)

print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

# Calculate confusion matrix
cm = confusion_matrix(y_true_np, y_pred_np)
print("\nConfusion Matrix:")
print(cm)

"""## Summary:

### Data Analysis Key Findings

*   The full dataset consists of 284,807 samples and 30 features.
*   The dataset was split into training (80%, 227,845 samples) and testing (20%, 56,962 samples) sets, preserving the class distribution in both sets.
*   Class weights were calculated to address the imbalance, with a significantly higher weight assigned to the minority class (fraud).
*   The model was trained for 100 epochs using the BCEWithLogitsLoss with the positive class weight applied.
*   On the test set, the model achieved a Precision of 0.3789, a Recall of 0.8776, and an F1-score of 0.5292.
*   The confusion matrix shows 56723 True Negatives, 86 True Positives, 141 False Positives, and 12 False Negatives.

### Insights or Next Steps

*   While the high recall (0.8776) indicates the model is good at identifying fraudulent transactions, the lower precision (0.3789) suggests a significant number of legitimate transactions are being flagged as fraudulent. This trade-off between precision and recall is common in imbalanced datasets and fraud detection.
*   Explore further techniques to improve the balance between precision and recall, such as adjusting the prediction threshold, using different loss functions, or exploring alternative sampling methods (e.g., SMOTE) on the training data.

"""