# mvdvc: Minimum Viable Data Version Control

**mvdvc** is a lightweight, educational CLI tool designed to demonstrate the core principles of data versioning and pipeline orchestration in Machine Learning projects, inspired by DVC (Data Version Control).

## Overview

This tool solves two major problems in ML projects:
1.  **Data Versioning**: Tracking large datasets and models using Git, without storing the actual large files in Git.
2.  **Pipeline Orchestration**: Managing dependencies between data processing code and data files, ensuring reproducibility and efficient incremental execution.

### Why it exists?
To understand the internal mechanisms of tools like DVC by building them from scratch. It prioritizes clarity and educational value over production readiness.

## Data Versioning

### How it works
`mvdvc` uses a content-addressable storage (CAS) model.

1.  **Hashing**: When you `mvdvc add <file>`, the tool calculates the MD5 hash of the file content.
2.  **Cache**: The file is copied to `.mvdvc/cache/<md5_hash>`. This directory acts as the local object store.
3.  **Pointer Files**: A small YAML file `<filename>.mvdvc` is created in your workspace. This file contains the hash and metadata.
    ```yaml
    path: data/raw/creditcard.csv
    hash: md5:abc123...
    size: 150MB
    type: file
    ```
4.  **Git Integration**: You commit the `.mvdvc` pointer file to Git. The actual large data file is ignored by Git (added to `.gitignore` manually or by convention).

### Remotes
`mvdvc` supports pushing and pulling data to remote storage.
- **Push**: Uploads objects from local `.mvdvc/cache` to the remote path.
- **Pull**: Downloads objects from the remote path to local `.mvdvc/cache`.
- **Checkout**: Copies data from `.mvdvc/cache` to your workspace based on the pointer files.

Currently, only **local filesystem remotes** are supported.

## Pipeline Orchestration

`mvdvc` orchestrates ML pipelines using a DAG (Directed Acyclic Graph) defined in `pipeline.yaml`.

### Pipeline YAML
The pipeline is defined in `pipeline.yaml`. Each stage has:
- `cmd`: The command to run.
- `deps`: Input files or code logic that the stage depends on.
- `outs`: Output files generated by the stage.

Example:
```yaml
stages:
  load:
    cmd: python3 -m src.load_data
    deps:
      - data/raw/creditcard.csv
      - src/load_data.py
    outs:
      - data/interim/clean.csv

  preprocess:
    cmd: python3 -m src.preprocess
    deps:
      - data/interim/clean.csv
      - src/preprocess.py
    outs:
      - data/processed/X_train.npy
```

### Incremental Execution
`mvdvc repro` ensures that stages are only run when necessary.

1.  **Signatures**: For each stage, a signature is calculated:
    `MD5(command + hash(dependencies))`
2.  **State**: Signatures are stored in `.mvdvc/pipeline_state.json`.
3.  **Logic**:
    - If the calculated signature matches the stored signature AND outputs exist -> **SKIP**.
    - If signature differs or outputs missing -> **RUN**.

This implies that if you change the code (`src/preprocess.py`) or the input data (`data/interim/clean.csv`), the signature changes, triggering a rerun of that stage and all downstream stages.

## How to Run the Demo

### Prerequisites
- Python 3.9+
- Dependencies installed: `pip install -r requirements.txt`
- `pyyaml` installed.

### Data Versioning Demo
Run the automated script:
```bash
bash demo_data_versioning.sh
```
What it does:
1.  Initializes `mvdvc`.
2.  Adds `data/raw/creditcard.csv`.
3.  Pushes to a local remote `/tmp/mvdvc_remote`.
4.  Deletes the local file.
5.  Restores it using `checkout`.

### Pipeline Orchestration Demo
Run the automated script:
```bash
bash demo_pipeline.sh
```
What it does:
1.  Runs the full pipeline (load -> preprocess -> train -> evaluate).
2.  Runs it again (skips all stages).
3.  Modifies `src/preprocess.py`.
4.  Runs again (reruns preprocess, train, and evaluate).

## Limitations
- **No Directory Support**: `add` only works reliably on single files.
- **No S3/Cloud Support**: Only local remotes.
- **Simple DAG**: Topological sort is basic.
- **No Locking**: Not thread-safe.

## Simplifications vs Real DVC
- DVC uses stronger hashing and optimization.
- DVC supports many remote types (S3, GCS, Azure, SSH).
- DVC has advanced dependency graph resolution and parameter tracking.
- DVC handles directory hashing with Merkle trees.
